# Ingenii Azure Data Platform dbt Example
An example dbt repository for using with [Ingenii's Azure Data Platform Databricks Runtime](https://github.com/ingenii-solutions/azure-data-platform-databricks-runtime).

Here we keep a record of the required settings for dbt to work with the platform. This repository is tested to work with dbt version 0.19.1.

## Usage

### Prerequisites
Your dbt projects needs to be held in an Azure DevOps repository. Create this ahead of working with this repository.

### Start from this repository
One approach is to clone this repository, copy all these files into your new Azure DevOps repository, and then update from this starting point.

### Initialise and update
Instead of clonging this repository, you can intialise your own dbt project in the Azure DevOps repository with the command `dbt init <project name>`, make the configuration changes listed below, and continue from this starting point.

### Configuration additions

`profiles.yml`:
 - This file stays in the root of the project, rather than in a `/home/<username>/.dbt` folder. This is for simplification.
 - `host`, `token`, and `cluster` are drawn from environment variables that are either set by our infrastructure deployment or python package. The profiles.yml file must be set this way, but you don't need to provide any of these variables.
 - `threads` is set to 2, while the default is 5. We've found that when performing a lot of dbt tests the cluster seems to get overloaded and tests will randomly fail. You're free to increase this number, but be warned.
 - One configuration entry is the `retry_all` flag, which at time of writing is specific to Ingenii's extended version of dbt-spark==0.19.1. More details are given below.

`dbt_project.yml`:
 - `profile`: This must be set to `databricks` to match the entry in profiles.yml.
 - `log-path`: This must be set to `/tmp/dbt-logs` so logs generated by dbt tests are moved correctly.

#### retry_all flag

When running dbt tests, when there are a lot of tests associated with a table we've found that a few will fail randomly due to intermittent connection issues. At time of writing dbt-spark will only retry a request if it deems the error it receives 'retryable' - usually when the message refers to the cluster starting up.

We have created a fix to allow users to retry all errors when testing, which this flag sets. At time of writing [our PR to update dbt-spark is open](https://github.com/dbt-labs/dbt-spark/pull/194).

### Schema additions

We have an example schema.yml in `models/random_data/` to showcase what we need to integrate dbt with Databricks and the Ingenii data engineering pipelines. Details of the files/tables we want to ingest are set here as `sources`, which are read by our pipelines to be able to get them into the Data Platform environment. 

For the general schema, please see the [dbt documentation for sources](https://docs.getdbt.com/reference/source-properties), and set or add the below changes for each `table` entry:
 - `external`: This should always be present and set as an object `using: "delta"` to integrate with the Azure Data Lake
 - `join`: A new object to define how we should add new data to the main table. The `column` entry will add a comma-separated list of names if your primary key involves more than one column. We have a few options for the `type`:
    - `merge_insert`: This will only insert new rows, as long as the entry does not already exist based on the `column` entry.
    - `merge_update`: This will insert new rows and update existing ones, matching based on the `column` entry.
    - `insert`, or not set: all rows in each file will be inserted, regardless if this causes a duplicate.
 - `file_details`: Gives details about the source file to help read it. We are expecting a falt.csv file, so these entries are passed to, and so must follow the conventions of, the [pyspark.sql.DataFrameReader.csv](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv) function. `path` and `schema` are set separately, so do not set these or the `inferSchema` and `enforceSchema` parameters. Some example parameters are below:
      - `sep`: When reading the source files, this is the field separator. For example, in comma-separated values (.csv), this is a comma ','
      - `header`: boolean, whether the source files have a header row
      - `dateFormat`: The format to convert from strings to date types. 
      - `timestampFormat`: The format to convert from strings to datetimes types.
 - `columns`: To read the csv file we set the data type using the new `data_type` key. For the values, we use the [Databricks SQL data types](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-datatypes#sql)

## dbt Resources:
- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)
- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers
- Join the [chat](http://slack.getdbt.com/) on Slack for live discussions and support
- Find [dbt events](https://events.getdbt.com) near you
- Check out [the blog](https://blog.getdbt.com/) for the latest news on dbt's development and best practices
